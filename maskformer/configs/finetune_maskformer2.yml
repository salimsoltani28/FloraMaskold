data:
  batch_size: 32 #in case of multiprocessing, this is single-worker batch size
  pin_memory: true
  num_workers: 1
  save_dataset: False
  shuffle_buffer: 10000
  seed: 0
  image_dir : '/mnt/gsdata/projects/panops/Labeled_data_seprated_in_Folder/image_all'
  mask_dir: '/mnt/gsdata/projects/panops/Labeled_data_seprated_in_Folder/image_mask_all'
  # image_dir: '/mnt/gsdata/users/soltani/Workshop_home_fromSSD2/Workshop_home/4_Mask2Former/data/image_all'
  # mask_dir: '/mnt/gsdata/users/soltani/Workshop_home_fromSSD2/Workshop_home/4_Mask2Former/data/image_mask_all'

model:
  type: MaskFormer2
  
train:
  start_epoch: 0
  epochs: 100
  # Need to check below parameters
  warmup_epochs: 20
  weight_decay: 0.05
  warmup_lr: 0.001
  min_lr: 0.0001
  base_lr: 0.0005  # This is the base learning rate for the model
  
  clip_grad: 5.0 

  lr_scheduler:
    name: cosine
    cycle_limit: 1
    decay_epochs: 80

  finetune:
    #If finetune is true, we load the model from the resume path and train few layers
    img_encoder_params_prefix: ['dummys']


  optimizer:
    name: adamw
    eps: 1e-8
    betas: [0.9, 0.999]
    base_lr: 0.0005


evaluate:
  #This check would disable loading of optimizer and scheduler if set to true
  pre_eval: false # Eval before training starts, useful for debugging
  eval_only: false
  eval_freq: 1
  save_best: true

checkpoint:
  auto_resume: true
  resume: ''
  loadonlymodel: false
  evaluate_checkpoint: false
  freq: 1
  max_kept: -1
  save_freq: 10
  

model_name: '' # display name in the logger
output: ???
wandb_output: ''
tag: default
print_freq: 1
seed: 0
wandb: true
local_rank: ???
vis: []